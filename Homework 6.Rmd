---
title: "Data Science Homework 6"
author: "Boya Guo"
date: "11/27/2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(modelr)
library(stats)
library(purrr)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_bw())
set.seed(1)
```

### Problem 1
```{r read in homicide data}
homicide = read_csv(file = "./data/homicide-data.csv") %>% 
  janitor::clean_names() 
```

First, we tidy the data, modifiy and change the variable names 
```{r p1.1}
homicide_1 = homicide %>% 
   mutate(victim_race = fct_relevel(ifelse(victim_race == "White", "white", "non-white"), "white"),
         victim_age = ifelse(victim_age == "Unknown", NA, as.integer(victim_age)),
         victim_sex = as.factor(victim_sex),
         city_state = paste(paste0(city, ","), state),
         resolved = as.numeric(disposition == "Closed by arrest")) %>% 
  filter(!city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL")) %>% 
  select(uid, victim_race, victim_age, victim_sex, city_state, resolved)
```

Next we create a logistic regression model for Baltimore and save the output of glm as an R object
```{r p1.2}
baltimore_logistic = homicide_1 %>%
  filter(city_state == "Baltimore, MD") %>% 
  glm(resolved ~ victim_age + victim_sex + victim_race, data = ., family = binomial()) 

save(baltimore_logistic,file = "baltimore_logistic.RData") 

baltimore_logistic
```

Then, we applied broom::tidy and obtain the estimate and CI for the adjusted OR 
```{r p1.3}
baltimore_logistic %>% 
  broom::tidy() %>% 
    mutate(OR = exp(estimate),
           conf_low = exp(estimate - 1.96*std.error),
           conf_high = exp(estimate + 1.96*std.error)) %>% 
    filter(term == "victim_racenon-white") %>% 
    select(term, OR, conf_low, conf_high) %>% 
    knitr::kable(digits = 4) 
```
The OR estimate for solving homicides comparing non-white victims to white victims is 0.4406 and the true OR lies between 0.3129 and 0.6204.


Next step, we run glm for each of the cities and extract the adjusted OR with CI

To do this, we create a logistic function and then perform the logistic regression on all cities
```{r p1.4}
city_logistic = function(x){
    homicide_1 %>% 
    filter(city_state == x) %>% 
    glm(resolved ~ victim_age + victim_sex + victim_race, data = ., family = binomial())  %>% 
    broom::tidy() %>% 
    mutate(OR = exp(estimate),
           conf_low = exp(estimate - 1.96 * std.error),
           conf_high = exp(estimate + 1.96 * std.error)) %>% 
    filter(term == "victim_racenon-white") %>% 
    select(term, OR, conf_low, conf_high)
}

cities = 
  tibble(city_state = unique(homicide_1$city_state)) %>% 
  mutate(map(.x = unique(homicide_1$city_state), ~city_logistic(.x))) %>% 
  unnest

cities
```

Create a plot that shows the estimated ORs and CIs for each city
```{r p1.5}
cities %>% 
  ggplot(aes(x = reorder(city_state, OR), y = OR, colour = city_state)) + 
  geom_point() +
  geom_errorbar(aes(ymin = conf_low, ymax = conf_high, width = 0.9)) +
  labs(
    title = "The estimated odds ratios and confidence intervals for solving homicides comparing non-white victims to white victims for each city",
    x = "City",
    y = "Odds Ratio and Confidence Interval"
    ) + 
  theme(legend.position = "right", axis.text.x = element_text(angle = 90, size = 7))
```
According to the plot created, we found that the mean odds ratio of solving for a non-white victim case compared to white victime is less than 1. This suggests that the non-white victim cases are more likely to be unsolved. In addition, we found that Boston has the smallest OR, Tampa has the largest OR, and Durham has the largest CI.


### Problem 2
```{r read in birthweight data}
birthweight = read_csv(file = "./data/birthweight.csv") %>%
  janitor::clean_names()  
```

```{r tidy data}
birthweight %>% 
  janitor::clean_names() %>% 
  na.omit() %>% 
  mutate(babysex = as.factor(babysex),
         frace = as.factor(frace),
         malform = as.factor(malform),
         mrace = as.factor(mrace))

birthweight
```
There was no missing data in the dataset, and the distribution of the variables looks normal. However, all records of pnumlbw and variable pnumsga are 0.


Then we could use stepwise selection to select our models
```{r p2.1}
full_model = lm(bwt ~ ., data = birthweight)

stepwise = step(full_model, direction = "both", trace = 0)
```

Then, we searched literature and tried to find which predictors we should include in our model 
```{r p2.2}
final_model = lm(bwt ~ babysex + bhead + blength + gaweeks + mheight + mrace + parity, data = birthweight)

summary(final_model)
```
We included the sex of baby, baby’s head circumference at birth, baby’s length at birth, gestational age in weeks, mother’s height, mother’s race, and parity into our final regression model based on lit review.


Then we created a plot of model residuals against fitted values 
```{r p2.3}
birthweight %>% 
  add_residuals(final_model) %>% 
  add_predictions(final_model) %>% 
  ggplot(aes(x = pred, y = resid)) + 
  geom_point(alpha = 0.5) +
  labs(
     title = "Residuals against fitted values",
     x = "Fitted value (prediction)",
     y = "Residual"
   )
```
According to the plot, we found that the residuals bounce randomly above and below the line residual = 0. The points roughly form a "horizontal band" around the line residual = 0. There were no obvious outliers in the plot. Therefore, We can assume that it met the criteria of a regression model.


Next, we compare your model to two others: 
One using length at birth and gestational age as predictors (main effects only);
One using head circumference, length, sex, and all interactions
```{r}

```

